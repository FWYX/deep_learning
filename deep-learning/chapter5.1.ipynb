{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0290,  0.2218,  0.0502,  0.1124,  0.0923,  0.0416, -0.0531,  0.0760,\n",
       "          0.0937,  0.1511],\n",
       "        [-0.0668,  0.1883,  0.1136, -0.0185,  0.0382,  0.0250,  0.0229,  0.0667,\n",
       "          0.0487,  0.2592]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#net是一个对象，是nn.Module的子类，有__call__方法，所以可以当做函数调用\n",
    "#net(X)相当于net.__call__(X)\n",
    "#callable可以看某个对象是否可以被当做函数调用\n",
    "callable(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPL(nn.Module):\n",
    "    def __init__(self):\n",
    "        #调用MPL的父类nn.Module的构造函数来执行必要的初始化\n",
    "        #这样在类实例化时也可以指定其他函数参数，例如模型参数params\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    #定义模型的前向传播\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1074,  0.0541, -0.0712,  0.0651,  0.0820,  0.1443, -0.0138,  0.1423,\n",
       "          0.0105,  0.0869],\n",
       "        [ 0.0422,  0.0738, -0.2088,  0.0511, -0.0014,  0.2440, -0.0857,  0.1450,\n",
       "          0.0571,  0.0467]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MPL()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        #这里我用列表来储存层的，但是书上让用字典\n",
    "        self.blocks = []\n",
    "        for block in args:\n",
    "            self.blocks.append(block)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for block in self.blocks:\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential()\n"
     ]
    }
   ],
   "source": [
    "#用列表储存层是打印不出来net的各个层的\n",
    "#应该是nn.Module中__str__()函数的原因\n",
    "#也就是说，用字典储存层是为了配合nn.Module中的其他函数\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = net(X)\n",
    "y.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Linear(in_features=20, out_features=256, bias=True),\n",
       "  ReLU(),\n",
       "  Linear(in_features=256, out_features=10, bias=True)],\n",
       " Parameter containing:\n",
       " tensor([[-0.1624,  0.0558, -0.0061,  ..., -0.0964, -0.0113, -0.0331],\n",
       "         [-0.2236,  0.1921,  0.0209,  ..., -0.1294,  0.1859,  0.1862],\n",
       "         [ 0.1358, -0.0953, -0.0246,  ...,  0.0627,  0.0805,  0.0752],\n",
       "         ...,\n",
       "         [-0.1585,  0.1208,  0.1648,  ...,  0.1761,  0.0515,  0.0729],\n",
       "         [ 0.1926, -0.1652, -0.1113,  ..., -0.1022,  0.0847, -0.0972],\n",
       "         [-0.1218, -0.1662, -0.1272,  ...,  0.0881, -0.0791, -0.1411]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = []\n",
    "blocks.append(nn.Linear(20, 256))\n",
    "blocks.append(nn.ReLU())\n",
    "blocks.append(nn.Linear(256, 10))\n",
    "blocks, blocks[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1750,  0.0244,  0.0492,  0.2723, -0.0485, -0.0486, -0.0341,  0.1803,\n",
       "         -0.0604,  0.1274],\n",
       "        [ 0.1393,  0.0010, -0.0430,  0.2210, -0.1947, -0.2531, -0.1204,  0.3382,\n",
       "         -0.0273,  0.0317]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = X.clone()\n",
    "for block in blocks:\n",
    "    result = block(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_init__函数将每个模块逐个添加到有序字典_modules中。 \\n读者可能会好奇为什么每个Module都有一个_modules属性？ \\n以及为什么我们使用它而不是自己定义一个Python列表？ \\n简而言之，_modules的主要优点是： \\n在模块的参数初始化过程中， 系统知道在_modules字典中查找需要初始化参数的子块。'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#书上是这样的：\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "'''_init__函数将每个模块逐个添加到有序字典_modules中。 \n",
    "读者可能会好奇为什么每个Module都有一个_modules属性？ \n",
    "以及为什么我们使用它而不是自己定义一个Python列表？ \n",
    "简而言之，_modules的主要优点是： \n",
    "在模块的参数初始化过程中， 系统知道在_modules字典中查找需要初始化参数的子块。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#用字典储存层就可以打印出来层\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[-0.0261, -0.0910, -0.0083,  0.0641, -0.0784,  0.1091, -0.0460, -0.2048,\n",
       "                        0.2164, -0.1356, -0.1360,  0.0683, -0.0943, -0.1578, -0.0382,  0.1487,\n",
       "                        0.2083, -0.2040,  0.0709, -0.0639],\n",
       "                      [-0.0916, -0.2222, -0.1142,  0.1466,  0.0556,  0.1170, -0.2057, -0.2130,\n",
       "                        0.1738, -0.2113,  0.0837,  0.1612, -0.0467,  0.0051, -0.0691,  0.1401,\n",
       "                       -0.0967, -0.1489, -0.1231, -0.1285],\n",
       "                      [ 0.0582,  0.2057, -0.1993,  0.2191, -0.0774,  0.1106,  0.2058,  0.1353,\n",
       "                       -0.0669,  0.2225,  0.1395, -0.0626, -0.1654, -0.0642,  0.1524, -0.0398,\n",
       "                       -0.0823,  0.0006, -0.1297, -0.0405],\n",
       "                      [ 0.0352,  0.0861, -0.1645, -0.0623, -0.1900, -0.1139,  0.0946,  0.1224,\n",
       "                       -0.1157, -0.1246,  0.0346,  0.1926, -0.0433,  0.0785,  0.2113,  0.0808,\n",
       "                       -0.1439,  0.0413, -0.1589, -0.0545],\n",
       "                      [ 0.1367, -0.0416,  0.0041, -0.2099,  0.0778, -0.2156, -0.0225, -0.1894,\n",
       "                        0.1244, -0.1883,  0.1612,  0.0768,  0.0063, -0.2076,  0.1833, -0.2225,\n",
       "                       -0.0573, -0.0477,  0.1330, -0.0798],\n",
       "                      [ 0.1423,  0.0355,  0.1924,  0.0581, -0.2224,  0.0700,  0.1071,  0.1627,\n",
       "                        0.0318,  0.0446,  0.1720,  0.1802,  0.1102,  0.0926, -0.0094, -0.0448,\n",
       "                       -0.0095,  0.1107,  0.0399,  0.0443],\n",
       "                      [ 0.0746,  0.1925,  0.1298,  0.0692, -0.0721, -0.2122,  0.1031,  0.0695,\n",
       "                        0.0905,  0.2049, -0.1016,  0.0990, -0.0029, -0.0937, -0.1274, -0.0316,\n",
       "                       -0.1680, -0.0497,  0.2034,  0.2133],\n",
       "                      [-0.1326,  0.1269, -0.0086,  0.1093, -0.1946, -0.1132,  0.1729, -0.1343,\n",
       "                        0.2220,  0.0136, -0.1795,  0.0481, -0.0657,  0.2108, -0.0722,  0.1414,\n",
       "                        0.2000,  0.2032, -0.1681, -0.0035],\n",
       "                      [-0.1035,  0.2104, -0.1920,  0.1117, -0.1142,  0.1851,  0.0839, -0.0791,\n",
       "                       -0.1071,  0.1317, -0.0692, -0.1020, -0.0945,  0.0445,  0.1192,  0.0968,\n",
       "                        0.1971, -0.1051, -0.2140, -0.1414],\n",
       "                      [ 0.0404, -0.0744,  0.1212,  0.0147,  0.1123, -0.2091,  0.1274, -0.1165,\n",
       "                       -0.1779, -0.0457, -0.0231, -0.1412,  0.1226, -0.0939,  0.1444,  0.0430,\n",
       "                       -0.0829,  0.1852,  0.1244, -0.1937],\n",
       "                      [-0.0428, -0.1769, -0.0697,  0.1816, -0.1421,  0.1571, -0.0517, -0.1790,\n",
       "                        0.0695, -0.0247,  0.0960,  0.1060,  0.1752,  0.0760,  0.1725, -0.1918,\n",
       "                        0.0209, -0.0885, -0.0111,  0.0753],\n",
       "                      [-0.0870, -0.0452,  0.1115,  0.0415, -0.1665,  0.0482,  0.0928,  0.1852,\n",
       "                       -0.0288, -0.0451, -0.0487,  0.0877,  0.0734, -0.0587, -0.1649, -0.0502,\n",
       "                       -0.1987, -0.1938,  0.1308,  0.0400],\n",
       "                      [ 0.0935, -0.1901,  0.1285,  0.1225, -0.1523,  0.1872, -0.0869, -0.0961,\n",
       "                        0.2182, -0.1455,  0.0348,  0.0459,  0.0723,  0.2162,  0.0122,  0.0868,\n",
       "                        0.1978, -0.0038, -0.2230,  0.1356],\n",
       "                      [-0.2185, -0.2027, -0.1115,  0.1687,  0.0719, -0.2210,  0.0097, -0.1393,\n",
       "                       -0.1479,  0.0836,  0.0323, -0.0913, -0.1824,  0.0058, -0.1966,  0.1953,\n",
       "                        0.0171, -0.1024, -0.0328,  0.1030],\n",
       "                      [ 0.0768, -0.1267, -0.0599, -0.0675,  0.0626,  0.0552,  0.1948, -0.1457,\n",
       "                       -0.0913, -0.2110,  0.1367, -0.2057, -0.1579, -0.0589, -0.0979,  0.0895,\n",
       "                        0.0406, -0.1608, -0.1304,  0.0045],\n",
       "                      [-0.0067,  0.0054,  0.2230,  0.1271,  0.1933,  0.0927,  0.0556,  0.1535,\n",
       "                        0.1252, -0.1542,  0.0437,  0.1283,  0.1469,  0.0441, -0.0901,  0.0009,\n",
       "                       -0.2000,  0.0227, -0.1470, -0.0580],\n",
       "                      [-0.0281, -0.1430, -0.0250,  0.0711,  0.0870, -0.0328, -0.0376,  0.0106,\n",
       "                       -0.1501, -0.0432,  0.0191,  0.0945, -0.0525, -0.1025, -0.1071, -0.0661,\n",
       "                       -0.0871,  0.1184, -0.0044,  0.1307],\n",
       "                      [ 0.0914,  0.0664,  0.0028,  0.0562, -0.1683, -0.1149,  0.1663, -0.2178,\n",
       "                       -0.0621,  0.0708,  0.1871,  0.1898,  0.1553,  0.1700, -0.1064,  0.1449,\n",
       "                       -0.1367,  0.1926, -0.0588,  0.0400],\n",
       "                      [-0.1404, -0.0454,  0.0252, -0.1015,  0.2024, -0.0503, -0.0205, -0.1162,\n",
       "                       -0.1281,  0.1710, -0.2089, -0.0818, -0.0287, -0.1199,  0.1107, -0.0164,\n",
       "                       -0.0930,  0.1806,  0.0910, -0.0308],\n",
       "                      [ 0.1259,  0.1370,  0.1119, -0.0223,  0.0095,  0.1595, -0.0257,  0.0152,\n",
       "                        0.1184,  0.1350, -0.1695, -0.0425,  0.0695,  0.0075,  0.2053, -0.0151,\n",
       "                       -0.1560,  0.2062, -0.0055, -0.1876]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0789,  0.2164,  0.0004,  0.0605, -0.1250,  0.0174,  0.0500, -0.1384,\n",
       "                      -0.0661,  0.1799,  0.1328,  0.0340,  0.2150, -0.1202, -0.2023,  0.1649,\n",
       "                      -0.1987,  0.2143, -0.1899,  0.0077]))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0169,  0.1424, -0.1518,  ...,  0.0480, -0.0597, -0.1075],\n",
      "        [ 0.2097,  0.2059, -0.1069,  ...,  0.0249, -0.0938,  0.0879],\n",
      "        [ 0.2004,  0.0726,  0.1240,  ...,  0.0409, -0.1526, -0.0526],\n",
      "        ...,\n",
      "        [ 0.1639, -0.0624, -0.0996,  ..., -0.1505,  0.1624,  0.0756],\n",
      "        [-0.1191, -0.1694,  0.1929,  ..., -0.1619,  0.1548, -0.1295],\n",
      "        [-0.1487,  0.2046,  0.0508,  ..., -0.2040,  0.1116, -0.1737]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0049, -0.1167,  0.2088, -0.0014,  0.1691,  0.2093,  0.1897,  0.0798,\n",
      "         0.2109, -0.1295, -0.0762, -0.0910, -0.1965,  0.0270, -0.1057,  0.0457,\n",
      "         0.1269, -0.1882, -0.0112,  0.0924,  0.0826,  0.1745, -0.1948, -0.1042,\n",
      "         0.1448, -0.0194,  0.0637, -0.0856, -0.1919, -0.1870,  0.1220, -0.0128,\n",
      "         0.0967,  0.0030,  0.1070,  0.1645, -0.0795, -0.1394,  0.1089,  0.1501,\n",
      "         0.1717,  0.1272, -0.0787,  0.0428,  0.2186, -0.0477, -0.1626, -0.1337,\n",
      "        -0.1966, -0.0346, -0.0928,  0.1895, -0.0568, -0.0846, -0.0719, -0.1285,\n",
      "        -0.2039,  0.1079,  0.2094,  0.0763,  0.1691, -0.0672, -0.0415, -0.1289,\n",
      "        -0.1185, -0.1943,  0.0723, -0.0056, -0.0476, -0.1893, -0.0501, -0.0041,\n",
      "        -0.1081,  0.1406, -0.2081, -0.1847,  0.1672,  0.0467,  0.1643, -0.2038,\n",
      "        -0.0279,  0.1928,  0.0307,  0.1176, -0.0351, -0.1916,  0.1194, -0.0520,\n",
      "         0.0873,  0.0942, -0.0965, -0.2079, -0.1405,  0.0381, -0.0027,  0.1791,\n",
      "        -0.1951,  0.1663,  0.0029,  0.1219, -0.1505, -0.1364,  0.1568, -0.1743,\n",
      "         0.1530, -0.2007, -0.1206,  0.0454,  0.1770, -0.0355, -0.2093, -0.0222,\n",
      "         0.1068, -0.2231, -0.2123, -0.1949,  0.0930,  0.1800,  0.1618,  0.1388,\n",
      "         0.0169,  0.1579, -0.1826,  0.0408, -0.0124, -0.1276, -0.0774,  0.0731,\n",
      "        -0.1852, -0.1830, -0.0036, -0.0199,  0.1923, -0.0019,  0.0228, -0.1207,\n",
      "        -0.0172, -0.1474, -0.0689, -0.0489, -0.0997,  0.1297,  0.0902,  0.1803,\n",
      "        -0.0272,  0.1564,  0.1784,  0.0797,  0.1285, -0.0007,  0.0096,  0.0718,\n",
      "        -0.0761,  0.1243,  0.0967, -0.1217, -0.1593,  0.1768,  0.0811,  0.0288,\n",
      "         0.0712, -0.1638,  0.2173,  0.1894,  0.0503, -0.0340, -0.1196,  0.0196,\n",
      "         0.0986,  0.0827,  0.0048,  0.0567, -0.0110, -0.1859,  0.0855, -0.0625,\n",
      "         0.1387, -0.0472,  0.1290,  0.0723,  0.1082, -0.1142,  0.0670,  0.0786,\n",
      "         0.1798, -0.0525,  0.0534, -0.1712, -0.0933,  0.1331, -0.2083,  0.0523,\n",
      "        -0.1877, -0.0091,  0.2231,  0.0655,  0.2198, -0.1974, -0.1826,  0.1945,\n",
      "        -0.0614,  0.0431,  0.0520, -0.1862,  0.0428,  0.0025,  0.0370, -0.1825,\n",
      "         0.1426,  0.0253,  0.1223, -0.1607, -0.0614, -0.1439, -0.0141,  0.0733,\n",
      "         0.0613, -0.0646,  0.1722,  0.0454,  0.0458, -0.1375, -0.0496, -0.1139,\n",
      "        -0.1032, -0.1485,  0.2005, -0.1487, -0.0908, -0.1917,  0.1237,  0.0167,\n",
      "         0.0556, -0.1955,  0.1348, -0.1460, -0.1744,  0.2109, -0.1344,  0.0427,\n",
      "         0.0135, -0.0137, -0.0210, -0.0255, -0.1916,  0.1177,  0.0366,  0.1481,\n",
      "         0.2157, -0.1876,  0.0706, -0.0981,  0.2092,  0.1129,  0.1214, -0.1884],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0172,  0.0174, -0.0243,  ...,  0.0132, -0.0604, -0.0313],\n",
      "        [-0.0182,  0.0062, -0.0607,  ..., -0.0366, -0.0339, -0.0100],\n",
      "        [-0.0093,  0.0276, -0.0337,  ..., -0.0462, -0.0505,  0.0286],\n",
      "        ...,\n",
      "        [-0.0286,  0.0376,  0.0226,  ...,  0.0066, -0.0579,  0.0294],\n",
      "        [ 0.0039, -0.0210,  0.0190,  ...,  0.0449,  0.0413,  0.0096],\n",
      "        [ 0.0095,  0.0178, -0.0602,  ...,  0.0177,  0.0366,  0.0295]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0182,  0.0152, -0.0012, -0.0549,  0.0099, -0.0042,  0.0362,  0.0625,\n",
      "         0.0160, -0.0031], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMPL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #不计算梯度的随机权重参数，在训练中保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        #使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        #复用全连接层。相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        #控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0459, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMPL()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestMPL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0090, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimera = nn.Sequential(NestMPL(), nn.Linear(16, 20), FixedHiddenMPL())\n",
    "chimera(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e4f9e2b429d5082852bebba048ae59313251508f681ce2db91105e3803a5035"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
